{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Agressive Tweets Classification RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sxzowbvjr6dt"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "BEfore doing anything **Switch to GPU**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2gMRbn0Zvsm6"
      },
      "source": [
        "# Prepare your environment "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mWCYuCdSrHao",
        "colab": {}
      },
      "source": [
        "# IMPORTS (try to organize/group your imports)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from os import path\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import sklearn.metrics\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, \\\n",
        "                            recall_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Pjxy_DO1VVBu",
        "colab": {}
      },
      "source": [
        "# Any global variables\n",
        "SEED = 15\n",
        "DATA_PATH = '/tmp/'\n",
        "MAX_SEQ_LEN = 40\n",
        "nlp = spacy.load('en')\n",
        "DEVICE = 'cuda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_FCp4JKKsduN",
        "colab": {}
      },
      "source": [
        "# Set SEEDs\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nIgXsF_6vnLs"
      },
      "source": [
        "# Examine and Prepare the Data\n",
        "\n",
        "\n",
        "## In deep learning it is not very often that we load the whole dataset into memory, especially the text portion of datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GT3nWZJtv4bc",
        "colab": {}
      },
      "source": [
        "# Download the data and save into raw_data.txt\n",
        "import requests\n",
        "url = \"https://raw.githubusercontent.com/w-is-h/DeepLearningNLP-Medical/master/Session_5/data/tweets.json\"\n",
        "response = requests.get(url, stream=True)\n",
        "\n",
        "# Save the dataset into a file\n",
        "f_raw_data = open(path.join(DATA_PATH, 'raw_data.txt'), 'wb')\n",
        "f_raw_data.write(response.content)\n",
        "f_raw_data.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H4_rWM75Wp7W",
        "outputId": "4826c748-2372-493d-c3fb-c6503329f0aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# To checkout files we now use bash commands\n",
        "!head /content/raw_data.txt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "head: cannot open '/content/raw_data.txt' for reading: No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j5WXv-mY0A5M",
        "colab": {}
      },
      "source": [
        "f_x_raw = open(path.join(DATA_PATH, 'x_raw.txt'), 'w')\n",
        "# We load labels because they are small\n",
        "y = []\n",
        "\n",
        "\n",
        "for line in open(path.join(DATA_PATH, 'raw_data.txt'), 'r'):\n",
        "  # Each line is in fact a json document\n",
        "  doc = json.loads(line) \n",
        "\n",
        "  # TODO: Write text to the file and append labels to 'y',\n",
        "  #each row must contain the text of one tweet\n",
        "  f_x_raw.write(\"{}\\n\".format(doc['content']))\n",
        "  y.append(int(doc['annotation']['label'][0]))\n",
        "\n",
        "# Close the file\n",
        "f_x_raw.close()\n",
        "\n",
        "# This is a typical way to add sanity checks to your code, can be very helpful.\n",
        "assert type(y[0]) == int"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SO_6oD_xgOHz"
      },
      "source": [
        "### Before cleaning we should analyse the dataset and understand what to remove or keep, but I've already done that so we skip it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ax8Lrwvv3yt4",
        "colab": {}
      },
      "source": [
        "# TODO: Cleaning\n",
        "# Every time a character (excluding numbers) is repeated more than 2 times, \n",
        "# reduce to 2 - e.g. \"0000 yesssssssss!!!!!!\" -> \"0000 yess!!\"\n",
        "def clean_text(text):\n",
        "  clean_text = re.sub(r'([^0-9]{1})\\1{2,}', r'\\1\\1', text)\n",
        "  return clean_text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qjYEC4kB0s63",
        "outputId": "7d2f7fdc-f1fb-4d6c-da89-c152c9a5479a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test the clean_text function\n",
        "test_text = \"0000 yesssssss!!!!!!\"\n",
        "test_out = clean_text(test_text)\n",
        "print(test_out)\n",
        "\n",
        "real_out = \"0000 yess!!\"\n",
        "assert real_out == test_out"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0000 yess!!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "avAo39Tb74u-"
      },
      "source": [
        "# Download Word Embeddings\n",
        "\n",
        "It is very rare to train your own embeddings, if your domain is not exteremly specific. Usually we use pretrained embeddings.\n",
        "\n",
        "In this case we are going to use embeddings from GloVe: Global Vectors for Word Representation. They have pretrained vectors for twitter datasets. \n",
        "\n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "The downside of doing this is that we can't continue the trainig of the vectors unless they are in the gensim word2vec format. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "More info at: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "syTY4srd-kJh",
        "outputId": "f4540997-d31d-455f-ce7b-6fad3701b37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "# JUPYTER/COLAB ONLY!!!\n",
        "# Download the data\n",
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-10-07 21:44:34--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2019-10-07 21:44:34--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2019-10-07 21:44:35--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  19.5MB/s    in 75s     \n",
            "\n",
            "2019-10-07 21:45:50 (19.3 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n",
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Er4vmxdi_eLe",
        "outputId": "6bbfe6e3-5fd5-48f7-94a3-1f943ca32bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Load the vectors\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# Convert glove file to word2vec format\n",
        "glove_file = datapath('/content/glove.twitter.27B.200d.txt')\n",
        "tmp_file = get_tmpfile(\"tmp_word2vec.txt\")\n",
        "_ = glove2word2vec(glove_file, tmp_file)\n",
        "\n",
        "# Load the newly generated file\n",
        "model = KeyedVectors.load_word2vec_format(tmp_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PiESc4WsDpBp",
        "outputId": "785ed241-0128-4656-9366-b055ba7ee26d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# Sanity - Check similarity \n",
        "model.most_similar(\"house\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('room', 0.799299418926239),\n",
              " ('home', 0.7727779746055603),\n",
              " ('apartment', 0.7143650054931641),\n",
              " ('party', 0.7122235894203186),\n",
              " ('out', 0.6893113255500793),\n",
              " ('my', 0.683701753616333),\n",
              " ('dad', 0.680922269821167),\n",
              " (\"'s\", 0.6800175309181213),\n",
              " ('going', 0.6792358756065369),\n",
              " ('up', 0.6730260848999023)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "av6z5kuuBmXp",
        "colab": {}
      },
      "source": [
        "embeddings = [] # A list of embeddings for each word in the word2vec vocab\n",
        "\n",
        "# Embeddings is a list, meaning we know that embeddings[1] is a vector for the \n",
        "#word with ID=1, but we don't know what word is that. That is why we need \n",
        "#the id2word and word2id mappings.\n",
        "id2word = {}\n",
        "word2id = {}\n",
        "\n",
        "# Loop over all words in the vocabulary and add the values\n",
        "for word in model.vocab.keys():\n",
        "  id2word[len(embeddings)] = word\n",
        "  word2id[word] = len(embeddings)\n",
        "  embeddings.append(model[word])\n",
        "\n",
        "# Add <UNK> and <PAD>\n",
        "word = \"<UNK>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.random.rand(len(embeddings[0])))\n",
        "word = \"<PAD>\"\n",
        "id2word[len(embeddings)] = word\n",
        "word2id[word] = len(embeddings)\n",
        "embeddings.append(np.zeros(len(embeddings[0])))\n",
        "\n",
        "# TODO: Convert the embeddings list into a numpy array\n",
        "#embeddings = 0#?\n",
        "\n",
        "# Convert the embeddings list into a tensor\n",
        "embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
        "\n",
        "# Sanity\n",
        "assert len(embeddings) == len(id2word) == len(word2id)\n",
        "assert model['house'][0] == embeddings[word2id['house']][0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EXbEK9VOFtkG"
      },
      "source": [
        "# Convert words to integers\n",
        "\n",
        "Usually we don't want to keep our input in the string format, it is very time/memory costly to load text all the time. We want to convert text into integers. That is why we have our mapping `word2id`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fumksoMuGDFA",
        "colab": {}
      },
      "source": [
        "x_ind = []\n",
        "for text in open(path.join(DATA_PATH, 'x_raw.txt')):\n",
        "  # TODO: clean text\n",
        "  text = clean_text(text.strip())\n",
        "  # Covnert text to lowercased tokens, skip punct and white-space\n",
        "  tkns = [tkn.lower_ for tkn in nlp.tokenizer(text) if not tkn.is_punct and\n",
        "          len(tkn.lower_.strip()) > 0]\n",
        "  # Convert each token into its id\n",
        "  ind_tkns = [word2id.get(tkn, word2id.get(\"<UNK>\")) for tkn in tkns]\n",
        "  # Append to x_ind\n",
        "  x_ind.append(ind_tkns)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XlEwkAh8VTyU",
        "outputId": "425512cd-fd5a-4866-9cdc-ddba72dce424",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(x_ind[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[147, 32, 124, 2567, 124, 109, 243, 26, 45, 80773, 1193514, 13, 25700, 70, 55, 408, 22480, 33, 41, 11, 1697, 183, 15927, 273, 63]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Gv5eSfp4c_f2",
        "outputId": "9bb2803d-25ad-4a66-9436-ab7be5f9b7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# TODO: convert the indexes for x_ind[1] back to words\n",
        "\" \".join([id2word[i] for i in x_ind[1]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"she is as dirty as they come and that crook <UNK> the dems are so fucking corrupt it 's a joke make republicans look like\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "I_tr32vkdH94"
      },
      "source": [
        "# Analyse the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OhKFnOtrhKwc",
        "outputId": "758f7e4c-188f-443e-80bb-32e187079f3e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "#calculate all the statistics\n",
        "tweet_lengths = [len(tweet) for tweet in x_ind]\n",
        "pos = np.sum(y) # Number of positive examples\n",
        "neg = len(y) - pos # Number of negative examples \n",
        "avg = np.average(tweet_lengths) # Average tweet length\n",
        "md = np.median(tweet_lengths) # Median tweet length\n",
        "mx = np.max(tweet_lengths) # Maximum tweet length\n",
        "mi = np.min(tweet_lengths) # Minimum tweet length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(neg))\n",
        "print(\"Number of negative examples: {}\".format(pos))\n",
        "print(\"Average length of the tweets: {}\".format(avg))\n",
        "print(\"Median length of the tweets: {}\".format(md))\n",
        "print(\"Max length of the tweets: {}\".format(mx))\n",
        "print(\"Min length of the tweets: {}\".format(mi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 12179\n",
            "Number of negative examples: 7822\n",
            "Average length of the tweets: 13.369781510924454\n",
            "Median length of the tweets: 12.0\n",
            "Max length of the tweets: 363\n",
            "Min length of the tweets: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CthdgmpzUv7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Calculate the tweet lengths, if length > 40 set to 40, if length == 0 set to 1\n",
        "prim_tweet_lens = [] # Append twet lengths here\n",
        "for tweet in x_ind:\n",
        "  if len(tweet) == 0:\n",
        "    prim_tweet_lens.append(1)\n",
        "  elif len(tweet) > 40:\n",
        "    prim_tweet_lens.append(40)\n",
        "  else:\n",
        "    prim_tweet_lens.append(len(tweet))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "G-SqG_957EiR"
      },
      "source": [
        "# Prepare the dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UfAKFrxZkP8d",
        "outputId": "e436cd9f-7688-4f0c-8de4-4f862ca61328",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Pad everything to the same length, or remove the extra\n",
        "x_ind_pad = []\n",
        "for i in range(len(x_ind)):\n",
        "  tweet = x_ind[i]\n",
        "  tweet = tweet[0:MAX_SEQ_LEN]\n",
        "  if len(tweet) < MAX_SEQ_LEN:\n",
        "    tweet.extend([word2id['<PAD>']] * (MAX_SEQ_LEN - len(tweet)))\n",
        "  x_ind_pad.append(tweet)\n",
        "\n",
        "# Print the stats again\n",
        "tweet_lengths = [len(tweet) for tweet in x_ind_pad]\n",
        "pos = np.sum(y) # Number of positive examples\n",
        "neg = len(y) - pos # Number of negative examples \n",
        "avg = np.average(tweet_lengths) # Average tweet length\n",
        "md = np.median(tweet_lengths) # Median tweet length\n",
        "mx = np.max(tweet_lengths) # Maximum tweet length\n",
        "mi = np.min(tweet_lengths) # Minimum tweet length\n",
        "\n",
        "print(\"Number of positive examples: {}\".format(neg))\n",
        "print(\"Number of negative examples: {}\".format(pos))\n",
        "print(\"Average length of the tweets: {}\".format(avg))\n",
        "print(\"Median length of the tweets: {}\".format(md))\n",
        "print(\"Max length of the tweets: {}\".format(mx))\n",
        "print(\"Min length of the tweets: {}\".format(mi))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positive examples: 12179\n",
            "Number of negative examples: 7822\n",
            "Average length of the tweets: 40.0\n",
            "Median length of the tweets: 40.0\n",
            "Max length of the tweets: 40\n",
            "Min length of the tweets: 40\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "T_PU9YGjo-Ff",
        "colab": {}
      },
      "source": [
        "# Split into train/test and move to pytorch \n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test, l_train, l_test = train_test_split(x_ind_pad, y, prim_tweet_lens, test_size=0.2, random_state=SEED)\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.long)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "l_train = torch.tensor(l_train, dtype=torch.long)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.long)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "l_test = torch.tensor(l_test, dtype=torch.long)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "k9U-7uYTCQnl"
      },
      "source": [
        "# Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GtJdSi1Xplft",
        "colab": {}
      },
      "source": [
        "class RNN(nn.Module):\n",
        "  def __init__(self, embeddings, padding_idx):\n",
        "    super(RNN, self).__init__()\n",
        "    # Get the required sizes\n",
        "    vocab_size = len(embeddings)\n",
        "    embedding_size = len(embeddings[0])\n",
        "    \n",
        "    # Initialize embeddings\n",
        "    self.embeddings = nn.Embedding(vocab_size, embedding_size, padding_idx=padding_idx)\n",
        "    self.embeddings.load_state_dict({'weight': embeddings})\n",
        "    # Disable training for the embeddings - IMPORTANT\n",
        "    self.embeddings.weight.requires_grad = False\n",
        "\n",
        "    hidden_size = 300\n",
        "\n",
        "    # Create the RNN cell\n",
        "    self.rnn = nn.RNN(input_size=200, hidden_size=hidden_size, num_layers=1, dropout=0.5)\n",
        "    self.fc1 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "  def forward(self, x, lns):\n",
        "    # Embed the input: from id -> vec\n",
        "    x = self.embeddings(x) # x.shape = batch_size x sequence_length x emb_size\n",
        "    \n",
        "    # Tell RNN to ignore padding and set the batch_first to True\n",
        "    x = torch.nn.utils.rnn.pack_padded_sequence(x, lns, batch_first=True, enforce_sorted=False) \n",
        "\n",
        "    # TODO: run 'x' through the RNN\n",
        "    x, hidden = self.rnn(x)\n",
        "\n",
        "    # Add the padding again\n",
        "    x, hidden = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "    \n",
        "    # For each example in batch select the value at the length of that sentence\n",
        "    row_indices = torch.arange(0, x.size(0)).long()\n",
        "    x = x[row_indices, lns-1, :]\n",
        "\n",
        "    # Push x through the fc network\n",
        "    x = self.fc1(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gEzGZJfm-RTF",
        "outputId": "4b444cb8-4192-4187-c6c9-49b6f926bc22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# TODO:\n",
        "device = torch.device(DEVICE) # Create a torch device\n",
        "net = RNN(embeddings, padding_idx=word2id['<PAD>']) # Create an instance of the RNN, take care what input parameters does it require\n",
        "criterion = nn.CrossEntropyLoss() # Set the criterion to Cross Entropy Loss\n",
        "parameters = filter(lambda p: p.requires_grad, net.parameters()) # Get only the parameters that require training\n",
        "optimizer = optim.Adam(parameters, lr=0.001) # Set the optimizer to Adam with lr = 0.001\n",
        "net.to(device) # Move the network to device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:51: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RNN(\n",
              "  (embeddings): Embedding(1193516, 200, padding_idx=1193515)\n",
              "  (rnn): GRU(200, 300, dropout=0.5)\n",
              "  (fc1): Linear(in_features=300, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D23v2YfGCWs3"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "PB9j6GSkCFeC",
        "outputId": "f3e57342-1791-459b-8377-d175cfb67f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "device = torch.device(DEVICE)\n",
        "# Move data to the right device only test, train is in batches\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "l_test = l_test.to(device)\n",
        "\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "\n",
        "batch_size = 1000\n",
        "\n",
        "# : calculate the number of batches given training size len(x_train)\n",
        "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
        "for epoch in range(80):\n",
        "  # : Switch network to train mode\n",
        "  net.train()\n",
        "\n",
        "  # Create the running loss array\n",
        "  running_loss = []\n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i+1) * batch_size\n",
        "    \n",
        "    # : Get the batch\n",
        "    x_train_batch = x_train[start:end] #?\n",
        "    y_train_batch = y_train[start:end] #?  \n",
        "    l_train_batch = l_train[start:end] #? \n",
        "\n",
        "    # : Move the batches to the right device\n",
        "    x_train_batch = x_train_batch.to(device) #?\n",
        "    y_train_batch = y_train_batch.to(device) #?\n",
        "    l_train_batch = l_train_batch.to(device) #?\n",
        "\n",
        "    # zero gradients\n",
        "    optimizer.zero_grad()\n",
        "    # Get outputs for our batch\n",
        "    outputs = net(x_train_batch, l_train_batch)\n",
        "    # Get loss\n",
        "    loss = criterion(outputs, y_train_batch)\n",
        "    # Do the backward step\n",
        "    loss.backward()\n",
        "    # Do the optimizer step\n",
        "    optimizer.step()\n",
        "\n",
        "    # Add the loss to the running_loss\n",
        "    running_loss.append(loss.item())\n",
        "\n",
        "  if epoch % 5 == 0:\n",
        "      net.eval()\n",
        "      outputs = net(x_train_batch, l_train_batch)\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in torch.max(outputs, 1)[1].cpu().detach().numpy()], y_train_batch.cpu().numpy())\n",
        "      outputs_dev = net(x_test, l_test)\n",
        "      acc_dev = sklearn.metrics.accuracy_score(torch.max(outputs_dev, 1)[1].cpu().detach().numpy(), y_test.cpu().numpy())\n",
        "      f1_dev = f1_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      p_dev = precision_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      r_dev = recall_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f} F1 Dev: {:.3f} p Dev: {:.3f} r Dev: {:.3f}\".format(epoch, np.average(running_loss), acc, acc_dev, f1_dev, p_dev, r_dev))\n",
        "      \n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.01831 Acc: 0.993 Acc Dev: 0.892 F1 Dev: 0.869 p Dev: 0.795 r Dev: 0.957\n",
            "Epoch:    5 Loss: 0.01600 Acc: 0.994 Acc Dev: 0.893 F1 Dev: 0.869 p Dev: 0.798 r Dev: 0.954\n",
            "Epoch:   10 Loss: 0.01479 Acc: 0.994 Acc Dev: 0.890 F1 Dev: 0.866 p Dev: 0.793 r Dev: 0.954\n",
            "Epoch:   15 Loss: 0.01408 Acc: 0.994 Acc Dev: 0.889 F1 Dev: 0.865 p Dev: 0.790 r Dev: 0.957\n",
            "Epoch:   20 Loss: 0.01387 Acc: 0.996 Acc Dev: 0.884 F1 Dev: 0.860 p Dev: 0.781 r Dev: 0.957\n",
            "Epoch:   25 Loss: 0.01411 Acc: 0.996 Acc Dev: 0.888 F1 Dev: 0.864 p Dev: 0.788 r Dev: 0.957\n",
            "Epoch:   30 Loss: 0.01415 Acc: 0.996 Acc Dev: 0.888 F1 Dev: 0.864 p Dev: 0.789 r Dev: 0.954\n",
            "Epoch:   35 Loss: 0.01229 Acc: 0.996 Acc Dev: 0.895 F1 Dev: 0.871 p Dev: 0.802 r Dev: 0.954\n",
            "Epoch:   40 Loss: 0.01217 Acc: 0.996 Acc Dev: 0.891 F1 Dev: 0.868 p Dev: 0.793 r Dev: 0.957\n",
            "Epoch:   45 Loss: 0.01201 Acc: 0.996 Acc Dev: 0.892 F1 Dev: 0.868 p Dev: 0.796 r Dev: 0.954\n",
            "Epoch:   50 Loss: 0.01192 Acc: 0.996 Acc Dev: 0.891 F1 Dev: 0.868 p Dev: 0.793 r Dev: 0.957\n",
            "Epoch:   55 Loss: 0.01169 Acc: 0.996 Acc Dev: 0.890 F1 Dev: 0.866 p Dev: 0.794 r Dev: 0.952\n",
            "Epoch:   60 Loss: 0.01163 Acc: 0.996 Acc Dev: 0.889 F1 Dev: 0.865 p Dev: 0.790 r Dev: 0.957\n",
            "Epoch:   65 Loss: 0.01142 Acc: 0.996 Acc Dev: 0.890 F1 Dev: 0.866 p Dev: 0.794 r Dev: 0.952\n",
            "Epoch:   70 Loss: 0.01137 Acc: 0.996 Acc Dev: 0.889 F1 Dev: 0.865 p Dev: 0.790 r Dev: 0.957\n",
            "Epoch:   75 Loss: 0.01120 Acc: 0.996 Acc Dev: 0.886 F1 Dev: 0.862 p Dev: 0.787 r Dev: 0.952\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqOGokjFZ1GT",
        "colab_type": "code",
        "outputId": "8319465c-f247-4e4a-b202-35228eeabf0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tweet = \"ahahaahahahaha yourr a funny kid\"\n",
        "print(tweet)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ahahaahahahaha\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cleknlh4nDnv",
        "colab_type": "code",
        "outputId": "9a68f0de-07e7-4570-eb0d-f0e11ada5ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tkns = [tkn.lower_ for tkn in nlp.tokenizer(tweet) if not tkn.is_punct and\n",
        "          len(tkn.lower_.strip()) > 0]\n",
        "print(tkns)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['ahahaahahahaha']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auBF-vsfnEth",
        "colab_type": "code",
        "outputId": "38183752-3ec6-4669-df91-286502383ca5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Convert tokens to indices \n",
        "inds = [word2id.get(tkn, word2id.get(\"<UNK>\")) for tkn in tkns]\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[935553]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMLbNu4vnXnG",
        "colab_type": "code",
        "outputId": "fe59f57e-20ea-42d8-8224-474beebac3c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Add padding to match len(inds) == 40\n",
        "inds.extend([word2id['<PAD>']] * (MAX_SEQ_LEN - len(inds)))\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[935553, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P49GDegynZ6s",
        "colab_type": "code",
        "outputId": "e9b39068-8cb9-4c75-e048-2c2170eaaa22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Move to torch\n",
        "inds = torch.tensor([inds]).to(device)\n",
        "print(inds)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 935553, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515,\n",
            "         1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515, 1193515]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLtkfPfenbx2",
        "colab_type": "code",
        "outputId": "3d88fa02-4a4c-42d5-ab99-d6f69b6c164e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Predict\n",
        "net.eval()\n",
        "torch.softmax(net(inds, torch.tensor([5])), dim=1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5767, 0.4233]], device='cuda:0', grad_fn=<SoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1yQIKqFniDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Get all predictions for x_test and apply softmax\n",
        "out = torch.softmax(net(x_test, l_test), dim=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5DDnjNOnokJ",
        "colab_type": "code",
        "outputId": "5813ac78-d833-4904-ff72-be084fefdbce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "# Find a couple of examples where the Net is sure it is correct\n",
        "out = out.detach().cpu().numpy()\n",
        "for i in range(200):\n",
        "  pred = np.argmax(out[i])\n",
        "  if pred != y_test[i] and out[i][pred] > 0.9:\n",
        "    print(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20\n",
            "22\n",
            "33\n",
            "39\n",
            "40\n",
            "86\n",
            "101\n",
            "104\n",
            "108\n",
            "110\n",
            "124\n",
            "135\n",
            "151\n",
            "183\n",
            "184\n",
            "189\n",
            "196\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilzwoAkBnrp4",
        "colab_type": "code",
        "outputId": "7ba176dc-01c6-41d1-ab59-346dc9f1f59f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Print the examples\n",
        "ind = 189\n",
        "print(y_test[ind])\n",
        "print(out[ind])\n",
        "print(\" \".join([id2word[i] for i in x_test[ind].cpu().detach().numpy() if id2word[i] != '<PAD>']))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(0, device='cuda:0')\n",
            "[2.8141444e-06 9.9999714e-01]\n",
            "ahahaahahahaha yourr a funny kid\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMXo8y57nuJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}