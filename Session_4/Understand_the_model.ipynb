{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Understand the model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdjVtLYL6sXJ",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_QSfQu7arNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SWITCH TO GPU\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import torch \n",
        "\n",
        "SEED = 15\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4lXNDq6njd",
        "colab_type": "text"
      },
      "source": [
        "# Get the data from github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0g6YMvNcFUA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/w-is-h/tmp/master/dataset.csv\", encoding='cp1252')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YALrViGcIQ7",
        "colab_type": "code",
        "outputId": "a1557fcb-da78-48ed-fb2f-80b856ec2177",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SentimentText</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>first think another Disney movie, might good, ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Put aside Dr. House repeat missed, Desperate H...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>big fan Stephen King's work, film made even gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>watched horrid thing TV. Needless say one movi...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>truly enjoyed film. acting terrific plot. Jeff...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       SentimentText  Sentiment\n",
              "0  first think another Disney movie, might good, ...          1\n",
              "1  Put aside Dr. House repeat missed, Desperate H...          0\n",
              "2  big fan Stephen King's work, film made even gr...          1\n",
              "3  watched horrid thing TV. Needless say one movi...          0\n",
              "4  truly enjoyed film. acting terrific plot. Jeff...          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO8wGAHN6uMq",
        "colab_type": "text"
      },
      "source": [
        "# Print some statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqoppNTg6xzu",
        "colab_type": "code",
        "outputId": "78d198ca-3c9f-4705-fa56-7ce5b992cc0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "shape = df.shape\n",
        "cols = list(df.columns) # Must be a list\n",
        "num_pos = np.sum(df['Sentiment'])\n",
        "print(\"The shape of the dataset is: \" + str(shape))\n",
        "print(\"The columns are: \" + str(cols))\n",
        "print(\"Number of positive values: \" + str(num_pos))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The shape of the dataset is: (25000, 2)\n",
            "The columns are: ['SentimentText', 'Sentiment']\n",
            "Number of positive values: 12500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YadlC2_cMzc",
        "colab_type": "code",
        "outputId": "db11a4a6-c28b-435f-f127-32cc46357162",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "_ = plt.hist(df['Sentiment'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaVcTC417YI8",
        "colab_type": "text"
      },
      "source": [
        "###Get the $x,y$ values from the datset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa66fNKWd2lT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = df['SentimentText'].values\n",
        "y = df['Sentiment'].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sErAVeYuduCx",
        "colab_type": "text"
      },
      "source": [
        "###Look at x and check do we need to clean anything\n",
        "\n",
        "Cleaning is not always necessary, but the more garbage we leave the more the Neural Network has to learn. So, if the cleaning process is easy - better to do it. Or get a gigantic dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aO4wpcINdWMz",
        "colab_type": "code",
        "outputId": "3342728d-864e-4f5b-ef27-fcfbe9bbd347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print(x[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. <br /><br />The characters composed one lame stereo-type another, obvious attempt creating another \"Bad News Bears\" embarrassing say least.<br /><br />I seen prized turkeys time, reason list since \"Numero Uno\".<br /><br />Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I9rNmcu6fpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove mails and https links\n",
        "pat_1 = r\"(?:\\@|https?\\://)\\S+\"\n",
        "# Remove tags\n",
        "pat_2 = r'#\\w+ ?'\n",
        "# Combine into one regex\n",
        "combined_pat = r'|'.join((pat_1, pat_2))\n",
        "# Remove websites\n",
        "www_pat = r'www.[^ ]+'\n",
        "# Remove HTML tags\n",
        "html_tag = r'<[^>]+>'\n",
        "def data_cleaner(text):\n",
        "  cleantags = \"\"\n",
        "  try:\n",
        "    stripped = re.sub(combined_pat, '', text)\n",
        "    stripped = re.sub(www_pat, '', stripped)\n",
        "    cleantags = re.sub(html_tag, '', stripped)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    cleantags = \"None\"\n",
        "  return cleantags"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98OEBCta7Os0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# : Run the cleaning function for each sentence in 'x' and save the results into a\n",
        "#new variable x2\n",
        "x2 = []\n",
        "for doc in x:\n",
        "  x2.append(data_cleaner(doc))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IoQ0ndefe_IF",
        "colab_type": "code",
        "outputId": "a63a2fa2-b150-41a9-a28c-6678a63aa0ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "x_original = x\n",
        "x = x2\n",
        "print(x[3])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "watched horrid thing TV. Needless say one movies watch see much worse get. Frankly, don't know much lower bar go. The characters composed one lame stereo-type another, obvious attempt creating another \"Bad News Bears\" embarrassing say least.I seen prized turkeys time, reason list since \"Numero Uno\".Let put way, watched Vanilla Ice movie, bad funny. This...this...is even good.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z75yCgk-7wLa",
        "colab_type": "text"
      },
      "source": [
        "# SpaCy\n",
        "\n",
        "We can use spacy to tokenize the text and further clean it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmE8oH7KbonG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import spacy\n",
        "from spacy.attrs import LOWER\n",
        "# Load the english model for spacy, the disable part is used to make it faster\n",
        "nlp = spacy.load('en', disable=['ner', 'parser'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3Jz7Ql5jCKr",
        "colab_type": "text"
      },
      "source": [
        "###The nlp variable is an instance of a class that has the method \\_\\_call__ defined, which means it is a class that is callable\n",
        "\n",
        "It returns an object that is a spacy representation of the input sentence\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iccllc0l7zHG",
        "colab_type": "code",
        "outputId": "e4e96837-e658-44d1-d234-168254616cca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "print(\"Type of the nlp variable is: \" + str(type(nlp)))\n",
        "doc = nlp(\"I was running yesterday.\")\n",
        "print(\"\\nThe 'nlp' function returns: \" + str(type(doc)))\n",
        "print(\"\\nIf we print the 'doc' variable we get the original input sentence: \" + str(doc))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Type of the nlp variable is: <class 'spacy.lang.en.English'>\n",
            "\n",
            "The 'nlp' function returns: <class 'spacy.tokens.doc.Doc'>\n",
            "\n",
            "If we print the 'doc' variable we get the original input sentence: I was running yesterday.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2gPwleAkTXb",
        "colab_type": "text"
      },
      "source": [
        "###To access tokens we can loop over the document - this means the 'doc' class has an \\_\\_iter__ method defined\n",
        "\n",
        "Look [here](https://github.com/explosion/spaCy/blob/9003fd25e5e966bd8d1b67a18f3ebd6010d6f718/spacy/tokens/doc.pyx) for the class definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2shYPjFX75Gq",
        "colab_type": "code",
        "outputId": "142229d8-5fc2-45b4-d7a7-28e81d6e8f29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "for token in doc:\n",
        "  print(token)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I\n",
            "was\n",
            "running\n",
            "yesterday\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ap-m0CAgoddy",
        "colab_type": "text"
      },
      "source": [
        "###Note that each token is a spacy object - not a string"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xicuAf3qoj_K",
        "colab_type": "code",
        "outputId": "925abe4a-9971-45eb-f25a-36b3d3727cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(type(doc[0])) # we can also index the doc object and get tokens"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'spacy.tokens.token.Token'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_ja7vJ3omkQ",
        "colab_type": "text"
      },
      "source": [
        "###To get strings we use the .text property"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw9y7IEeoq6W",
        "colab_type": "code",
        "outputId": "3112b0f3-dd2f-4669-81ad-1c2fbbc0e856",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(type(doc[0].text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lzY4iHv18CVt",
        "colab_type": "code",
        "outputId": "03e4786c-9465-4573-c5a6-1fbad5bf0944",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        }
      },
      "source": [
        "# Each token has multiple properties, e.g. lemma_ - note the '_', that means print text\n",
        "for token in doc:\n",
        "  print(token.lemma_)\n",
        "  print(token.is_stop)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-PRON-\n",
            "True\n",
            "\n",
            "be\n",
            "True\n",
            "\n",
            "run\n",
            "False\n",
            "\n",
            "yesterday\n",
            "False\n",
            "\n",
            ".\n",
            "False\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXbv-Df64G50",
        "colab_type": "code",
        "outputId": "7870bd53-b7a9-4a75-c0ee-0eb6d0066436",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "snt = \"That was a very good movie, John\"\n",
        "# : Tokenize the sentence above and save the lemmatized strings into the \n",
        "#tmp variable\n",
        "\n",
        "tmp = [t.lemma_ for t in nlp(snt)]\n",
        "print(tmp)\n",
        "# The output should be ['that', 'be', 'a', 'very', 'good', 'movie', ',', 'John']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['that', 'be', 'a', 'very', 'good', 'movie', ',', 'John']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUpxGw6vo2su",
        "colab_type": "code",
        "outputId": "0eae9467-d838-4c65-e2cf-bf628b7c5113",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# : Tokenize the sentence again and save the lemmatized strings into the \n",
        "#tmp variable, but skip stopwords and punctuation plus lowercase the lemmas, \n",
        "#print the tmp variable\n",
        "tmp = [t.lemma_.lower() for t in nlp(snt) if not t.is_stop and not t.is_punct]\n",
        "print(tmp)\n",
        "# The output should be ['good', 'movie', 'john']"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['good', 'movie', 'john']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ykAaAne8KVb",
        "colab_type": "text"
      },
      "source": [
        "# Split sentences into tokens and lowercase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81ry5hpe8XGp",
        "colab_type": "code",
        "outputId": "66cb6acb-c41c-46e4-b673-90fc01649fe2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYR6ueKeWz6",
        "colab_type": "code",
        "outputId": "f2371102-3ef5-4a5d-c428-ce7e66f62b6f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# : Lemmatize and split each sentence in our dataset 'x', save the new\n",
        "#lemmatized strings into the tok_snts variable. Skip punctuation and stopwords plus lowercase lemmas\n",
        "tok_snts = []\n",
        "for snt in x:\n",
        "  tkns = [tkn.lemma_.lower() for tkn in nlp(snt) if not tkn.is_punct]\n",
        "  tok_snts.append(tkns)\n",
        "# Save back\n",
        "x = tok_snts\n",
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'think', 'another', 'disney', 'movie', 'may', 'good', '-pron-', 'be', 'kid', 'movie', 'watch', '-pron-', 'can', 'not', 'help', 'enjoy', '-pron-', 'age', 'love', 'movie', 'first', 'see', 'movie', '10', '8', 'year', 'later', 'still', 'love', '-pron-', 'danny', 'glover', 'superb', 'could', 'play', 'part', 'better', 'christopher', 'lloyd', 'hilarious', 'perfect', 'part', 'tony', 'danza', 'believable', 'mel', 'clark', 'can', 'not', 'help', 'enjoy', 'movie', 'give', '10/10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQc1FRv48n9J",
        "colab_type": "text"
      },
      "source": [
        "# Train word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVSxGpV4cOx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(x, size=300, window=6, min_count=4, workers=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpMdj6WIUvS",
        "colab_type": "text"
      },
      "source": [
        "#Similarity\n",
        "\n",
        "We can now easily calculate word similarity using the `w2v.wv.most_similar()` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KyOvtimcsdh",
        "colab_type": "code",
        "outputId": "448cdfcc-55a8-414b-c3d4-4fa772048ae0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        }
      },
      "source": [
        "w2v.wv.most_similar(\"bad\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('terrible', 0.7558525800704956),\n",
              " ('awful', 0.742184579372406),\n",
              " ('worst', 0.7319484949111938),\n",
              " ('horrible', 0.7262938022613525),\n",
              " ('suck', 0.7081185579299927),\n",
              " ('crappy', 0.6620049476623535),\n",
              " ('stupid', 0.655234694480896),\n",
              " ('good', 0.6336821913719177),\n",
              " ('lame', 0.6173223257064819),\n",
              " ('lousy', 0.6149851679801941)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSMdm2yTIHmM",
        "colab_type": "text"
      },
      "source": [
        "#Get vectors\n",
        "\n",
        "To get a vector for a word 'w' we can use the `w2v.wv.get_vector(w)` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FU_X_Wb48tGW",
        "colab_type": "code",
        "outputId": "7b627168-c86b-4333-c9d0-a9b79e91a35e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#To check is a word in our vocab we use:\n",
        "if 'bad' in w2v.wv:\n",
        "  print(w2v.wv.get_vector(\"bad\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 8.58379304e-01  9.63060111e-02  1.67783082e+00 -6.64549544e-02\n",
            " -7.73063302e-01  1.30722892e+00 -9.64514017e-01  2.01369548e+00\n",
            " -6.95125043e-01 -1.01643217e+00 -3.00314814e-01 -4.02315054e-03\n",
            " -1.73157537e+00 -8.93085748e-02  3.88970703e-01  5.44557810e-01\n",
            " -9.32950616e-01 -5.98617673e-01 -2.27493897e-01  1.04499435e+00\n",
            " -1.30096745e+00 -6.59945607e-02  3.96612823e-01 -9.80607688e-01\n",
            "  4.92028266e-01  2.92546004e-01  2.03496113e-01  1.45969316e-01\n",
            "  6.43451452e-01  5.53118587e-01  1.13225734e+00  4.37913597e-01\n",
            "  1.59180641e+00  9.94859099e-01 -1.83274671e-02 -6.44646704e-01\n",
            "  1.39594340e+00 -3.73896331e-01  1.85349953e+00  3.48258048e-01\n",
            "  4.24019754e-01 -1.05870223e+00  3.65218818e-01  1.75368581e-02\n",
            "  2.13663027e-01 -1.80073977e-01 -6.20474160e-01  9.55948412e-01\n",
            "  2.90025562e-01 -1.55642891e+00  1.08075464e+00  1.23852897e+00\n",
            " -7.14166999e-01 -1.29446924e+00  1.40655935e+00 -3.34232777e-01\n",
            "  3.63631248e-01  1.31799686e+00  5.20931110e-02  2.65992731e-01\n",
            "  1.94021058e+00  6.93812072e-01  5.25328636e-01 -7.62119293e-02\n",
            " -3.42872053e-01 -3.73359412e-01  1.21805894e+00 -2.79656947e-01\n",
            "  1.38629401e+00 -1.13686585e+00  6.04384303e-01 -3.12048107e-01\n",
            " -1.49889901e-01 -3.56470227e-01  6.36984408e-01 -3.47799450e-01\n",
            "  1.33170152e+00  8.73962492e-02 -3.25648308e-01 -8.31842244e-01\n",
            "  1.20239604e+00 -9.19124007e-01  4.74270880e-01  3.16107422e-01\n",
            "  2.22235322e-01 -1.01058233e+00 -1.01105690e+00  1.49308160e-01\n",
            " -1.38573110e+00 -1.45278895e+00 -2.42489517e-01 -9.10226405e-02\n",
            " -8.60692382e-01 -7.12678954e-02 -1.22333539e+00 -2.33563557e-02\n",
            "  1.63298738e+00  9.70140457e-01 -1.09777331e+00 -2.41518810e-01\n",
            "  5.08920372e-01  6.89140141e-01 -1.16610849e+00  9.68757331e-01\n",
            " -3.59154284e-01  2.08338213e+00  5.28522432e-01  1.33908963e+00\n",
            "  1.50729597e+00  4.75905061e-01 -2.66753901e-02 -7.59644359e-02\n",
            " -1.16703296e+00 -1.14990854e+00  4.76756334e-01  1.21185815e+00\n",
            "  1.13388240e+00 -8.63550723e-01  1.26467097e+00 -1.14014208e-01\n",
            "  6.11426830e-01 -1.59761384e-01 -5.58691144e-01 -7.35499084e-01\n",
            "  1.12968795e-01  1.42126977e+00 -1.66119623e+00 -3.51815581e-01\n",
            " -1.82026066e-04  1.11377871e+00 -8.82256217e-03 -7.85074472e-01\n",
            "  2.74116486e-01  1.01999283e+00 -2.74395764e-01  3.35849702e-01\n",
            " -4.51949149e-01 -9.46420372e-01  3.30335468e-01  1.40743637e+00\n",
            "  1.44353449e+00  8.34206998e-01  5.60384691e-01  9.15796876e-01\n",
            "  9.11451340e-01  2.42440879e-01 -3.10429364e-01  3.58157679e-02\n",
            " -1.01644598e-01 -4.82486993e-01  7.40862727e-01  1.11169321e-02\n",
            "  8.12126994e-01 -2.03604326e-01 -9.54972088e-01 -5.19572616e-01\n",
            " -8.83625209e-01 -3.31805289e-01 -7.89384902e-01 -1.12971336e-01\n",
            " -7.06255496e-01  2.72586793e-01  3.57256413e-01 -2.53693551e-01\n",
            "  6.56462610e-01  1.25421271e-01  5.37055016e-01  1.06442451e+00\n",
            " -2.58997619e-01  7.87037015e-02 -5.23186922e-01 -2.75905401e-01\n",
            "  2.50316828e-01  2.18260549e-02 -5.97219765e-01  1.36958823e-01\n",
            "  2.26399317e-01  1.48620951e+00 -8.52519274e-01  2.15823174e-01\n",
            " -5.35832942e-01 -9.35349241e-02 -4.18275207e-01  6.90304637e-02\n",
            " -1.25699604e+00  1.20930707e+00  1.38863802e+00  9.10818756e-01\n",
            " -1.29693460e+00 -2.09978652e+00 -1.06519818e+00 -6.89545095e-01\n",
            "  1.27923238e+00  7.80947149e-01 -3.04169148e-01 -1.09585673e-01\n",
            " -1.12385184e-01 -3.77649069e-01  4.95810628e-01 -7.99095750e-01\n",
            "  5.79408109e-01  1.25166869e+00 -2.65433956e-02  7.83141732e-01\n",
            "  9.67894316e-01 -9.03749287e-01 -1.69145644e+00 -2.31135651e-01\n",
            " -3.09089959e-01  5.08927226e-01  2.08834201e-01 -2.79069424e-01\n",
            "  3.49838078e-01 -1.53578624e-01  4.51402485e-01 -3.01874757e-01\n",
            " -3.78500521e-01 -2.92181641e-01 -9.53495502e-01 -1.10053882e-01\n",
            " -1.99773204e+00  9.16050315e-01  7.55070448e-01 -9.84207869e-01\n",
            "  2.72708386e-01  7.34506100e-02 -7.00639009e-01 -8.28292489e-01\n",
            "  2.16232345e-01 -4.71439183e-01  5.07767022e-01  1.96070790e-01\n",
            "  8.56333435e-01  2.96194971e-01 -6.80615127e-01 -1.52359456e-02\n",
            "  2.12783217e-01  6.50524318e-01 -4.84862447e-01 -8.39036524e-01\n",
            " -4.64530766e-01  5.63363135e-01 -8.31398070e-01 -6.70285523e-01\n",
            "  3.05691987e-01 -3.42962325e-01 -3.17881465e-01  2.81325459e-01\n",
            " -2.87855655e-01 -2.16028050e-01  1.66887629e+00  7.64339464e-03\n",
            "  4.14898217e-01 -1.25276768e+00  4.61943328e-01 -9.29116786e-01\n",
            "  8.95247698e-01 -1.58698225e+00  1.81357130e-01 -4.96242106e-01\n",
            " -1.91843271e-01 -9.43115950e-02  6.64462805e-01  3.12954783e-01\n",
            " -7.19496906e-01  2.14121252e-01  1.43729234e+00  1.08059919e+00\n",
            " -4.33812648e-01  4.32207823e-01 -4.77423742e-02  5.18665314e-01\n",
            "  1.09432292e+00  3.83042783e-01  4.24421012e-01  1.98030263e-01\n",
            " -9.76362288e-01 -1.29111242e+00 -3.27880353e-01  4.67853576e-01\n",
            "  3.37985635e-01  1.08072171e-02  6.42682135e-01 -1.03281453e-01\n",
            "  6.05703712e-01 -1.21375358e+00  3.47510219e-01 -1.02477717e+00\n",
            " -1.51907012e-01 -1.16090691e+00 -5.32770336e-01  9.54447269e-01\n",
            "  9.51153934e-01 -3.96878988e-01 -9.33580518e-01 -9.28175002e-02\n",
            "  7.44909942e-01 -2.85501052e-02  5.01512587e-01 -5.27800143e-01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZsis4V88i-",
        "colab_type": "text"
      },
      "source": [
        "# Convert each sentence into the average sum of the vector representations of its tokens\n",
        "\n",
        "Save the results into a new variable x_emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTQktTzBdaYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# \n",
        "\n",
        "# x_emb - embedded sentences\n",
        "x_emb = np.zeros((len(x), 300))\n",
        "# Loop over sentences\n",
        "for i_snt, snt in enumerate(x):\n",
        "  cnt = 0\n",
        "  # Loop over the words of a sentence\n",
        "  for i_word, word in enumerate(snt):\n",
        "    if word in w2v.wv:\n",
        "      x_emb[i_snt] += w2v.wv.get_vector(word)\n",
        "      cnt += 1\n",
        "  if cnt > 0:\n",
        "    x_emb[i_snt] = x_emb[i_snt] / cnt\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXQJQFjdkUo6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get torch stuff\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sklearn.metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATUWE2b9VKd",
        "colab_type": "text"
      },
      "source": [
        "# Split the dataset into train/test/dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kpy4rEGnghM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "inds = np.random.permutation(len(x))\n",
        "inds_train = inds[0:int(0.8*len(x))]\n",
        "inds_test = inds[int(0.8*len(x)):int(0.9*len(x))]\n",
        "inds_dev = inds[int(0.9*len(x)):]\n",
        "\n",
        "# 80% of the dataset\n",
        "x_train = x_emb[inds_train]\n",
        "y_train = y[inds_train]\n",
        "x_train_w = np.array(x)[inds_train]\n",
        "x_train_o = np.array(x_original)[inds_train]\n",
        "\n",
        "# 10% of the dataset\n",
        "x_test = x_emb[inds_test]\n",
        "y_test = y[inds_test]\n",
        "\n",
        "# 10% of the dataset\n",
        "x_dev = x_emb[inds_dev]\n",
        "y_dev = y[inds_dev]\n",
        "\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "x_dev = torch.tensor(x_dev, dtype=torch.float32)\n",
        "y_dev = torch.tensor(y_dev.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test.reshape(-1, 1), dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH3ssL6_dK4",
        "colab_type": "text"
      },
      "source": [
        "#Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjDb8Wz0m2PU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "# : Set the device to 'cuda'\n",
        "device = torch.device('cuda')\n",
        "\n",
        "# : Build a network with 4 layers:\n",
        "#1: 150 neurons and ReLU activation (300 inputs)\n",
        "#2: 70 neurons and ReLU activation\n",
        "#3: 25 neurons and ReLU activation\n",
        "#4: 1 neuron and sigmoid activation\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 150)\n",
        "      self.fc2 = nn.Linear(150, 70)\n",
        "      self.fc3 = nn.Linear(70, 25)\n",
        "      self.fc4 = nn.Linear(25, 1)\n",
        "      \n",
        "      self.d1 = nn.Dropout(0.5)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.d1(torch.relu(self.fc1(x)))\n",
        "      x = self.d1(torch.relu(self.fc2(x)))\n",
        "      x = self.d1(torch.relu(self.fc3(x)))\n",
        "      x = torch.sigmoid(self.fc4(x))\n",
        "      return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMz8tBisAaGA",
        "colab_type": "code",
        "outputId": "42c6ee7c-18bf-4aa2-e86b-05aca6a961c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# Create the network and get BCE loss\n",
        "net = Net()\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "# : Make a SGD optimizer with lr=0.002 and momentum=0.99\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.99)\n",
        "# : Move the net to the device\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=300, out_features=150, bias=True)\n",
              "  (fc2): Linear(in_features=150, out_features=70, bias=True)\n",
              "  (fc3): Linear(in_features=70, out_features=25, bias=True)\n",
              "  (fc4): Linear(in_features=25, out_features=1, bias=True)\n",
              "  (d1): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7VRuEb9bj1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARtU1lLR_cfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Move data to the right device\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "\n",
        "x_dev = x_dev.to(device)\n",
        "y_dev = y_dev.to(device)\n",
        "\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXYUTXUoS-t",
        "colab_type": "code",
        "outputId": "838271c3-0f1f-4b0d-9026-069b4c648d1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "source": [
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "net.train()\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "for epoch in range(5000):  # do 10,000 epochs \n",
        "  # : zero the gradients on the optimizer\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # : Calculate the forward pass \n",
        "  outputs = net(x_train)\n",
        "  # : Calculate the loss\n",
        "  loss = criterion(outputs, y_train)\n",
        "  # : Backward pass\n",
        "  loss.backward()\n",
        "  # : Optimize/Update parameters\n",
        "  optimizer.step()\n",
        "  \n",
        "  # Track the changes - This is normally done using tensorboard or similar\n",
        "  losses.append(loss.item())\n",
        "  accs.append(sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy()))\n",
        "\n",
        "  # print statistics\n",
        "  if epoch % 500 == 0:\n",
        "      net.eval()\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_dev)\n",
        "      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "      accs_dev.append(acc_dev)\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "      net.train()\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.69641 Acc: 0.502 Acc Dev: 0.484\n",
            "Epoch:  500 Loss: 0.48869 Acc: 0.789 Acc Dev: 0.825\n",
            "Epoch: 1000 Loss: 0.39840 Acc: 0.834 Acc Dev: 0.840\n",
            "Epoch: 1500 Loss: 0.37760 Acc: 0.848 Acc Dev: 0.848\n",
            "Epoch: 2000 Loss: 0.35987 Acc: 0.855 Acc Dev: 0.854\n",
            "Epoch: 2500 Loss: 0.34690 Acc: 0.864 Acc Dev: 0.853\n",
            "Epoch: 3000 Loss: 0.33909 Acc: 0.871 Acc Dev: 0.854\n",
            "Epoch: 3500 Loss: 0.32450 Acc: 0.876 Acc Dev: 0.855\n",
            "Epoch: 4000 Loss: 0.31220 Acc: 0.882 Acc Dev: 0.850\n",
            "Epoch: 4500 Loss: 0.29809 Acc: 0.889 Acc Dev: 0.851\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhPQXS_uopWq",
        "colab_type": "code",
        "outputId": "84997da8-ee10-4570-a8b6-24ae1211329b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# Switch to eval mode \n",
        "net.eval()\n",
        "net.to(device)\n",
        "print(\"Predicted value: \" + str(net(torch.tensor(x_emb[4], dtype=torch.float32).to(device))))\n",
        "print(\"Real value: \" + str(y[4]))\n",
        "print(\"Input sentence: \" + df['SentimentText'].iloc[4])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted value: tensor([0.8787], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "Real value: 1\n",
            "Input sentence: truly enjoyed film. acting terrific plot. Jeff Combs talent recognized for. part flick would change ending. death creature far gruesome Sci Fi Channel.<br /><br />There interesting religious messages film. Jeff Combs obviously played Messiah figure creature (or shark prefer) represented anti-Chirst. particularly frightening scenes 'end world feel'. noticed third viewing classic creature feature. know many people won't get references Christianity, watch close you'll get it.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJsArQIGHz3B",
        "colab_type": "text"
      },
      "source": [
        "#Go back and add dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGjzLNow-T9_",
        "colab_type": "text"
      },
      "source": [
        "#Interpretability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2vaQdnM-Wyf",
        "colab_type": "code",
        "outputId": "47717361-025b-43f9-c97d-420eeebb7abe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "# Make a small network with one layer\n",
        "device = torch.device('cuda')\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 1)\n",
        "       \n",
        "    def forward(self, x):\n",
        "      x = torch.sigmoid(self.fc1(x))\n",
        "      return x\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.99)\n",
        "for epoch in range(4000):  # do 10,000 epochs \n",
        "  # zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # Forward \n",
        "  outputs = net(x_train)\n",
        "  # Calculate error\n",
        "  loss = criterion(outputs, y_train)\n",
        "  # Backward\n",
        "  loss.backward()\n",
        "  # Optimize/Update parameters\n",
        "  optimizer.step()\n",
        "  \n",
        "  # Track the changes - This is normally done using tensorboard or similar\n",
        "  losses.append(loss.item())\n",
        "\n",
        "  # print statistics\n",
        "  if epoch % 500 == 0:\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_dev)\n",
        "      acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.69390 Acc: 0.478 Acc Dev: 0.472\n",
            "Epoch:  500 Loss: 0.45045 Acc: 0.806 Acc Dev: 0.815\n",
            "Epoch: 1000 Loss: 0.41469 Acc: 0.824 Acc Dev: 0.832\n",
            "Epoch: 1500 Loss: 0.40022 Acc: 0.832 Acc Dev: 0.836\n",
            "Epoch: 2000 Loss: 0.39209 Acc: 0.835 Acc Dev: 0.836\n",
            "Epoch: 2500 Loss: 0.38684 Acc: 0.837 Acc Dev: 0.836\n",
            "Epoch: 3000 Loss: 0.38316 Acc: 0.838 Acc Dev: 0.840\n",
            "Epoch: 3500 Loss: 0.38043 Acc: 0.840 Acc Dev: 0.839\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVx2mD7b_YZq",
        "colab_type": "code",
        "outputId": "1aa7fd61-50f1-444f-85b3-77521f95e671",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        }
      },
      "source": [
        "# Print the prediction of the first 30 sentences\n",
        "for i in range(30):\n",
        "  out = net(x_train[i])\n",
        "  print(i, out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 tensor([0.2364], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "1 tensor([0.2689], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "2 tensor([0.1942], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "3 tensor([0.1000], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "4 tensor([0.0066], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "5 tensor([0.1738], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "6 tensor([0.6825], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "7 tensor([0.0027], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "8 tensor([0.0459], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "9 tensor([0.0030], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "10 tensor([0.6532], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "11 tensor([0.2991], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "12 tensor([0.9461], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "13 tensor([0.7538], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "14 tensor([0.0054], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "15 tensor([0.3641], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "16 tensor([0.0024], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "17 tensor([0.1423], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "18 tensor([0.9083], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "19 tensor([0.0920], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "20 tensor([0.2999], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "21 tensor([0.2255], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "22 tensor([0.9897], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "23 tensor([0.3516], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "24 tensor([0.2889], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "25 tensor([0.9509], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "26 tensor([0.9780], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "27 tensor([0.0179], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "28 tensor([0.9633], device='cuda:0', grad_fn=<SigmoidBackward>)\n",
            "29 tensor([0.9114], device='cuda:0', grad_fn=<SigmoidBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCVkmeCE_iKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set the 'i_snt' to high prob\n",
        "i_snt = 25"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CeAWnVvA-r4h",
        "colab_type": "code",
        "outputId": "d81b0428-e08b-4295-cec1-986d0d46fef6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "prod = []\n",
        "# Take the weights of our layer\n",
        "we = net.fc1.weight.detach().cpu().numpy()[0]\n",
        "# For each word in the dataset check how much did it contribute to the \n",
        "#final classification\n",
        "for ind, word in enumerate(x_train_w[i_snt]):\n",
        "  if word in w2v:\n",
        "    prod.append(np.dot(we, w2v.wv.get_vector(word)))\n",
        "  else:\n",
        "    prod.append(0)\n",
        "    \n",
        "# Sort the contributions\n",
        "srt = np.argsort(prod)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
            "  import sys\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mewfla3Q_Gfi",
        "colab_type": "code",
        "outputId": "d8da7dc2-3a57-43de-89e6-5968bbc21aaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 618
        }
      },
      "source": [
        "print(x_train_o[i_snt].replace(\"<br /><br />\", \"\\n\"))\n",
        "print()\n",
        "# print the top 10 words that contributed the most\n",
        "for i in range(1, 30):\n",
        "  print(x_train_w[i_snt][srt[-i]], prod[srt[-i]])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1954 Marlon Brando hot actor performances Streetcar Named Desire Waterfront. Frank Sinatra yet re-invent silver screen. Sinatra's portrayal erstwhile Nathan Detroit, helped re-establish Sinatra fans.\n",
            "It great screen version great play choices leads support players terrific. Imagine movie Brando sings? one singing role portrayed Sky Masterson. addition female leads, Jean Simmons Vivian Blaine(replaying stage role Nathan's long suffering girlfriend Adelade), put superlative efforts. Special mention goes great Stubby Kaye(as Nicely Nicely), due respect Eric Clapton, one's version Rockin' Boat even comes close Stubby's. Sheldon Leonard, would go fame TV producer shows Danny Thomas Show Dick Van Dyke Show \"Harry Horse\" wonders, B.S.Pulley excellent harsh mannered rough talking \"Big Julie\", even Regis Toomey offers excellence \"Brother Arvide\".\n",
            "It one fun musicals see, good comedy, get Sinatra Brando. Soooooo \"Luck Lady Tonight\" brother...\"it's dice\"\n",
            "\n",
            "great 37.80559\n",
            "great 37.80559\n",
            "great 37.80559\n",
            "excellent 33.776993\n",
            "fan 25.009674\n",
            "fun 20.643145\n",
            "musical 17.059988\n",
            "good 16.140314\n",
            "frank 15.208466\n",
            "terrific 14.618347\n",
            "support 14.200927\n",
            "see 13.190943\n",
            "sky 12.241675\n",
            "sing 12.21904\n",
            "comedy 12.057147\n",
            "sinatra 11.933183\n",
            "sinatra 11.933183\n",
            "sinatra 11.933183\n",
            "sinatra 11.933183\n",
            "portrayal 11.832767\n",
            "performance 10.910128\n",
            "show 10.810721\n",
            "show 10.810721\n",
            "show 10.810721\n",
            "jean 10.456103\n",
            "nicely 9.369637\n",
            "nicely 9.369637\n",
            "yet 9.165161\n",
            "close 9.116967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GE-ACAsa9Bkh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "review1 = \"That was not a good movie and my friend was there\"\n",
        "review2 = \"That was a good movie and my friend was not there\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoAXnTuhAenx",
        "colab_type": "code",
        "outputId": "34a3149b-bf56-4c98-db45-07909494389b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(review1)\n",
        "# Tokenize the review using spacy - same as we have done above\n",
        "tkns = [tkn.lemma_.lower() for tkn in nlp(review1) if not tkn.is_punct]\n",
        "# Convert the tokens to vectors and save into an array\n",
        "vecs = [w2v.wv.get_vector(t) for t in tkns]\n",
        "emb = torch.tensor(np.average(vecs, axis=0)).to(device)\n",
        "net(emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That was not a good movie and my friend was there\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8754], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fycnlnhK7ReI",
        "colab_type": "code",
        "outputId": "9a8d934d-0aef-4adb-8dd4-0b9064bde51d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(review2)\n",
        "# Tokenize the review using spacy - same as we have done above\n",
        "tkns = [tkn.lemma_.lower() for tkn in nlp(review2) if not tkn.is_punct]\n",
        "# Convert the tokens to vectors and save into an array\n",
        "vecs = [w2v.wv.get_vector(t) for t in tkns]\n",
        "emb = torch.tensor(np.average(vecs, axis=0)).to(device)\n",
        "net(emb)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "That was a good movie and my friend was not there\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.8754], device='cuda:0', grad_fn=<SigmoidBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gYgqG2h7hPP",
        "colab_type": "code",
        "outputId": "00648560-172f-4491-807e-47cf194966dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        }
      },
      "source": [
        "# TODO: print 3 examples where the model made a mistake\n",
        "#x_train_o - contains the original text\n",
        "#x_train - contains the embeding for each review in x_train_o\n",
        "#y_train - contains the ground truth\n",
        "for i in range(len(x_train_o)):\n",
        "  text = x_train_o[i]\n",
        "  emb = x_train[i]\n",
        "  gt = y_train[i]\n",
        "  \n",
        "  pred = net(emb)\n",
        "  pred = pred.cpu().detach().numpy()[0]\n",
        "  \n",
        "  if pred < 0.01 and gt == 1:\n",
        "    print(text)\n",
        "    print()\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Actually liked movie very, much. it`s plot, acting, jokes, no. liked it, it`s one worse movies ever created. It`s lame, bad, becomes terribly funny. jokes actually cool, rest makes pray unemployment scriptwriter. \"Men white\" dumb stupid, two things. Turn TV roll floor laughing (beer helps lot:). chose second option.\n",
            "\n",
            "low rating? really don't get it... bad acting? bad dialogue? Well, cares things cheesy low-budget horror movies? Seriously, acting dialogue isn't important movies. People hate movies bad acting bad dialogue shouldn't allowed rate cheesy low-budget movies. movies shouldn't taken seriously. Period.<br /><br />Anyway, time talk movie, right? Well, loved it! bought expected gorefest, it's gorefest gore pretty bad (most time it's animal guts placed body actors that's lame), didn't really care movie hilarious! characters hilarious, acting hilarious (bad acting GOOD thing cheesy low-budget horror movies), dialogue hilarious (bad dialogue GOOD thing cheesy low-budget horror movies), zombie rapist huge dick hilarious, flying demon baby hilarious could go on, don't want say much... mention there's scene girl masturbates sex doll like it's alive lol! Oh zombie rapist falls love sex doll lol!<br /><br />Best lines movie:<br /><br />Detective Manners: *sniffs coke* Detective Sloane: *beep* doing, Manners? hell snort? hell that? Detective Manners: It's nothing man, it's... Ehh... Cold medicine...<br /><br />Detective Manners: *injects heroin arm* Detective Sloane: *beep* doing, Manners? *beep* insane? Detective Manners: It's cold medicine.<br /><br />Detective Manners: *repeatedly kicks random guy face* Detective Sloane: hell's going on, Manners? doing? Detective Manners: maniac rambling demons started smashing head rock! started smashing head rock! think he's PCP something!<br /><br />LOL!\n",
            "\n",
            "creators film made attempt introducing reality plot, would one waste time, money, creative effort. Fortunately, throwing pretense reality winds, created comedic marvel. could pass film alien pilot spends entire film acting like Jack Nicholson, complete Lakers T-shirt. dismiss film trash.\n",
            "\n",
            "flick sterling example state erotic B-movies: bad porn movies without hardcore sex. plot one isn't bad things go; involves female lawyer trying prove lover innocent killing wife. rest movie, however, leaves something desired. Bad acting, bad direction, bad looking woman, bad sets, bad cinematography, bad sound bad sex scenes. filmmakers learn difference raunchy erotic. don't even common sense Gabriella Hall naked love scene.<br /><br /> dumb that?\n",
            "\n",
            "simply funniest movie I've seen long time. bad acting, bad script, bad scenery, bad costumes, bad camera work bad special effects stupid find reeling laughter.<br /><br />So it's gonna win Oscar you've got beer friends round can't go wrong.\n",
            "\n",
            "Stupid, Stupid, Stupid. think Angelina Jolie probably one talented actress' today, movie like isn't worth time. deserves better, everyone else movie. Talent wasted. Sorry, don't feel like writing review this.<br /><br />I give stars *****.\n",
            "\n",
            "movie everything makes bad movie worth watching - sloppy editing, little continuity, insane dialog, bad (you might even say non-existent) acting, pointless story lines, shots go FAR long...and it's perfect MST3K-style riffing, mention \"Corpse Eaters Drinking Game\": Scribble forms...take shot - Sign name...take shot - Catch bad Foley edit...take many, many shots.<br /><br />The reason didn't rate higher 8 there's enough gratuitous nudity despite insane badness, it's hour long - hell, movie like least 20-30 minutes longer!\n",
            "\n",
            "definitive movie version Hamlet. Branagh cuts nothing, wasted moments.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKHaQHYmYitZ",
        "colab_type": "text"
      },
      "source": [
        "# Let's switch to batches"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xQdx8LmXcxH",
        "colab_type": "code",
        "outputId": "33770cb0-13b8-45e2-9f8c-b6e08d469653",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 757
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 1)\n",
        "       \n",
        "    def forward(self, x):\n",
        "      x = torch.sigmoid(self.fc1(x))\n",
        "      return x\n",
        "net = Net()\n",
        "net.to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.99)\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 1000\n",
        "num_batches = int(np.ceil(len(x_train) / batch_size)) #TODO: Calculate the number of batches based on x_train size and batch_size\n",
        "for epoch in range(4000):  # do 10,000 epochs \n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i+1) * batch_size  \n",
        "    x_train_batch = x_train[start:end] #? Get batch_size examples from x_train\n",
        "    y_train_batch = y_train[start:end] #? Get batch_size examples from y_train\n",
        "    # zero the gradients\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward \n",
        "    outputs = net(x_train_batch)\n",
        "    # Calculate error\n",
        "    loss = criterion(outputs, y_train_batch)\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "    # Optimize/Update parameters\n",
        "    optimizer.step()\n",
        "\n",
        "    # Track the changes - This is normally done using tensorboard or similar\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # print statistics\n",
        "    if epoch % 500 == 0:\n",
        "        outputs = net(x_train)\n",
        "        acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs.cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "\n",
        "        outputs_dev = net(x_dev)\n",
        "        acc_dev = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in outputs_dev.cpu().detach().numpy()], y_dev.cpu().numpy())\n",
        "\n",
        "        print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev))\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.70135 Acc: 0.426 Acc Dev: 0.424\n",
            "Epoch:    0 Loss: 0.70107 Acc: 0.427 Acc Dev: 0.425\n",
            "Epoch:    0 Loss: 0.70093 Acc: 0.427 Acc Dev: 0.425\n",
            "Epoch:    0 Loss: 0.70091 Acc: 0.429 Acc Dev: 0.425\n",
            "Epoch:    0 Loss: 0.69827 Acc: 0.431 Acc Dev: 0.426\n",
            "Epoch:    0 Loss: 0.69893 Acc: 0.433 Acc Dev: 0.428\n",
            "Epoch:    0 Loss: 0.69914 Acc: 0.435 Acc Dev: 0.430\n",
            "Epoch:    0 Loss: 0.69873 Acc: 0.437 Acc Dev: 0.434\n",
            "Epoch:    0 Loss: 0.69889 Acc: 0.442 Acc Dev: 0.440\n",
            "Epoch:    0 Loss: 0.69816 Acc: 0.446 Acc Dev: 0.447\n",
            "Epoch:    0 Loss: 0.69912 Acc: 0.450 Acc Dev: 0.454\n",
            "Epoch:    0 Loss: 0.69845 Acc: 0.455 Acc Dev: 0.458\n",
            "Epoch:    0 Loss: 0.69782 Acc: 0.459 Acc Dev: 0.462\n",
            "Epoch:    0 Loss: 0.69599 Acc: 0.465 Acc Dev: 0.467\n",
            "Epoch:    0 Loss: 0.69486 Acc: 0.470 Acc Dev: 0.475\n",
            "Epoch:    0 Loss: 0.69838 Acc: 0.475 Acc Dev: 0.482\n",
            "Epoch:    0 Loss: 0.69569 Acc: 0.480 Acc Dev: 0.488\n",
            "Epoch:    0 Loss: 0.69618 Acc: 0.485 Acc Dev: 0.494\n",
            "Epoch:    0 Loss: 0.69584 Acc: 0.490 Acc Dev: 0.502\n",
            "Epoch:    0 Loss: 0.69570 Acc: 0.495 Acc Dev: 0.507\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-32709b0f9198>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m# Optimize/Update parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJiVJCGjz4wn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}