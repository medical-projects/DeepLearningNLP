{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unbalanced datasets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdjVtLYL6sXJ",
        "colab_type": "text"
      },
      "source": [
        "#Sentiment Classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_QSfQu7arNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# SWITCH TO GPU\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import torch \n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "\n",
        "SEED = 15\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9s4lXNDq6njd",
        "colab_type": "text"
      },
      "source": [
        "# Get the data from github "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0g6YMvNcFUA",
        "colab_type": "code",
        "outputId": "658a5851-7197-40ba-928c-e5e3149264d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/w-is-h/tmp/master/dataset.csv\", encoding='cp1252')\n",
        "x = df['SentimentText'].values\n",
        "y = df['Sentiment'].values\n",
        "print(y)\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 0 1 ... 0 0 1]\n",
            "first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I9rNmcu6fpZ",
        "colab_type": "code",
        "outputId": "53f43d9d-619d-4497-a4c1-38ea6c2670db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Remove mails and https links\n",
        "pat_1 = r\"(?:\\@|https?\\://)\\S+\"\n",
        "# Remove tags\n",
        "pat_2 = r'#\\w+ ?'\n",
        "# Combine into one regex\n",
        "combined_pat = r'|'.join((pat_1, pat_2))\n",
        "# Remove websites\n",
        "www_pat = r'www.[^ ]+'\n",
        "# Remove HTML tags\n",
        "html_tag = r'<[^>]+>'\n",
        "def data_cleaner(text):\n",
        "  cleantags = \"\"\n",
        "  try:\n",
        "    stripped = re.sub(combined_pat, '', text)\n",
        "    stripped = re.sub(www_pat, '', stripped)\n",
        "    cleantags = re.sub(html_tag, '', stripped)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    cleantags = \"None\"\n",
        "  return cleantags\n",
        "\n",
        "x_original = x\n",
        "x = [data_cleaner(review) for review in x]\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first think another Disney movie, might good, it's kids movie. watch it, can't help enjoy it. ages love movie. first saw movie 10 8 years later still love it! Danny Glover superb could play part better. Christopher Lloyd hilarious perfect part. Tony Danza believable Mel Clark. can't help, enjoy movie! give 10/10!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QsHKKGBhVg-Q"
      },
      "source": [
        "# SpaCy\n",
        "\n",
        "We can use spacy to tokenize the text and further clean it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "joYR6ueKeWz6",
        "colab_type": "code",
        "outputId": "100c9b24-96ae-4a3f-a11c-af1c55794a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import spacy\n",
        "from spacy.attrs import LOWER\n",
        "# Load the english model for spacy, the disable part is used to make it faster\n",
        "nlp = spacy.load('en', disable=['ner', 'parser'])\n",
        "\n",
        "tok_snts = []\n",
        "for snt in x:\n",
        "  tkns = [tkn.lemma_.lower() for tkn in nlp.tokenizer(snt) if not tkn.is_punct]\n",
        "  tok_snts.append(tkns)\n",
        "# Save back\n",
        "x = tok_snts\n",
        "# Print the first sentence\n",
        "print(x[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['\\ufeff1', 'think', 'another', 'disney', 'movie', 'may', 'good', '-pron-', 'have', 'kid', 'movie', 'watch', 'it', 'can', 'not', 'help', 'enjoy', 'it', 'age', 'love', 'movie', '\\ufeff1', 'see', 'movie', '10', '8', 'year', 'late', 'still', 'love', 'it', 'danny', 'glover', 'superb', 'can', 'play', 'part', 'well', 'christopher', 'lloyd', 'hilarious', 'perfect', 'part', 'tony', 'danza', 'believable', 'mel', 'clark', 'can', 'not', 'help', 'enjoy', 'movie', 'give', '10/10']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQc1FRv48n9J",
        "colab_type": "text"
      },
      "source": [
        "# Train word2vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVSxGpV4cOx8",
        "colab_type": "code",
        "outputId": "f58e3e7c-cf90-44ee-f2dd-d7a73cc9bc54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "w2v = Word2Vec(x, size=300, window=6, min_count=4, workers=4)\n",
        "w2v.wv.most_similar(\"bad\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('awful', 0.7379966974258423),\n",
              " ('terrible', 0.7341678142547607),\n",
              " ('horrible', 0.7156305313110352),\n",
              " ('suck', 0.690376877784729),\n",
              " ('lousy', 0.6557600498199463),\n",
              " ('lame', 0.6505146026611328),\n",
              " ('ritchie', 0.6240953207015991),\n",
              " ('crappy', 0.6187602877616882),\n",
              " ('alright', 0.6176861524581909),\n",
              " ('stupid', 0.6132094860076904)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfZsis4V88i-",
        "colab_type": "text"
      },
      "source": [
        "# Convert each sentence into the average sum of the vector representations of its tokens\n",
        "\n",
        "Save the results into a new variable x_emb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTQktTzBdaYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_emb - embedded sentences\n",
        "x_emb = np.zeros((len(x), 300))\n",
        "# Loop over sentences\n",
        "for i_snt, snt in enumerate(x):\n",
        "  cnt = 0\n",
        "  # Loop over the words of a sentence\n",
        "  for i_word, word in enumerate(snt):\n",
        "    if word in w2v.wv:\n",
        "      x_emb[i_snt] += w2v.wv.get_vector(word)\n",
        "      cnt += 1\n",
        "  if cnt > 0:\n",
        "    x_emb[i_snt] = x_emb[i_snt] / cnt\n",
        "# Save the originals, will be needd later\n",
        "x_or = x_emb\n",
        "y_or = y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lATUWE2b9VKd",
        "colab_type": "text"
      },
      "source": [
        "# Split the dataset into train/test/dev"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoQ8UehuMkmm",
        "colab_type": "code",
        "outputId": "f66f216a-c317-4d59-823c-0c1d706cb3c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# TODO: Find the indices where y_or == 1 and y_or == 1\n",
        "inds_z = np.where(y_or == 0)[0]\n",
        "inds_o = np.where(y_or == 1)[0]\n",
        "print(inds_z)\n",
        "print(inds_o)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[    1     3     6 ... 24995 24997 24998]\n",
            "[    0     2     4 ... 24993 24996 24999]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_z2V-9-NBft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO: Create new x_emb and y, so that the new x_emb has 12500 negative examples and 1000 positive examples\n",
        "x_emb = x_or[np.concatenate((inds_z, inds_o[0:1000])), :]\n",
        "y = y_or[np.concatenate((inds_z, inds_o[0:1000]))]\n",
        "\n",
        "# TODO: Create x_one that has 1000 positive examples and x_zero that has 12500 negative examples\n",
        "x_one = x_or[inds_o[0:1000], :]\n",
        "x_zero = x_or[inds_z, :]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwu59SGeYKn5",
        "colab_type": "code",
        "outputId": "ea917743-0d4a-48fd-8bad-353e19c64fa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(y.shape)\n",
        "print(x_emb.shape)\n",
        "print(\"Number of positive examples in y: \" + str(np.sum(y)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13500,)\n",
            "(13500, 300)\n",
            "Number of positive examples in y: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEi8yXP5L0D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get torch stuff\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import sklearn.metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kpy4rEGnghM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(SEED)\n",
        "y = y.reshape(-1)\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_emb, y, test_size=0.2, random_state=SEED)\n",
        "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "x_one = torch.tensor(x_one, dtype=torch.float32)\n",
        "x_zero = torch.tensor(x_zero, dtype=torch.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZH3ssL6_dK4",
        "colab_type": "text"
      },
      "source": [
        "#Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjDb8Wz0m2PU",
        "colab_type": "code",
        "outputId": "194dee99-4b5e-4817-a67d-5a9e84cfdeb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "device = torch.device('cuda')\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "      super(Net, self).__init__()\n",
        "      self.fc1 = nn.Linear(300, 100)\n",
        "      self.fc4 = nn.Linear(100, 2)\n",
        "      \n",
        "      self.d1 = nn.Dropout(0.5)\n",
        "      \n",
        "    def forward(self, x):\n",
        "      x = self.d1(torch.relu(self.fc1(x)))\n",
        "      x = torch.sigmoid(self.fc4(x))\n",
        "      return x\n",
        "# Create the network and get CE loss\n",
        "net = Net()\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.tensor([0.1, 0.9]).to(device))\n",
        "# Make a SGD optimizer with lr=0.002 and momentum=0.99\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.02, momentum=0.99)\n",
        "# Move the net to the device\n",
        "net.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (fc1): Linear(in_features=300, out_features=100, bias=True)\n",
              "  (fc4): Linear(in_features=100, out_features=2, bias=True)\n",
              "  (d1): Dropout(p=0.5)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3E7VRuEb9bj1",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mXYUTXUoS-t",
        "colab_type": "code",
        "outputId": "a5c106bd-2b9b-4e3d-ab53-8329b2a58904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Move data to the right device\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "net.train()\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "for epoch in range(10000): \n",
        "  optimizer.zero_grad()\n",
        "  outputs = net(x_train)\n",
        "  loss = criterion(outputs, y_train)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if epoch % 500 == 0:\n",
        "      net.eval()\n",
        "      acc = sklearn.metrics.accuracy_score(torch.max(outputs, 1)[1].cpu().detach().numpy(), y_train.cpu().numpy())\n",
        "      \n",
        "      outputs_dev = net(x_test)\n",
        "      acc_dev = sklearn.metrics.accuracy_score(torch.max(outputs_dev, 1)[1].cpu().detach().numpy(), y_test.cpu().numpy())\n",
        "      accs_dev.append(acc_dev)\n",
        "      \n",
        "      # TODO: calculate the f1_score, precision and recall\n",
        "      f1_dev = f1_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      p_dev = precision_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      r_dev = recall_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f} F1 Dev: {:.3f} p Dev: {:.3f} r Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev, f1_dev, p_dev, r_dev))\n",
        "      net.train()\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.69230 Acc: 0.475 Acc Dev: 0.496 F1 Dev: 0.140 p Dev: 0.079 r Dev: 0.627\n",
            "Epoch:  500 Loss: 0.45132 Acc: 0.859 Acc Dev: 0.856 F1 Dev: 0.411 p Dev: 0.280 r Dev: 0.768\n",
            "Epoch: 1000 Loss: 0.43840 Acc: 0.877 Acc Dev: 0.861 F1 Dev: 0.417 p Dev: 0.288 r Dev: 0.757\n",
            "Epoch: 1500 Loss: 0.43202 Acc: 0.884 Acc Dev: 0.868 F1 Dev: 0.429 p Dev: 0.300 r Dev: 0.757\n",
            "Epoch: 2000 Loss: 0.42726 Acc: 0.888 Acc Dev: 0.873 F1 Dev: 0.439 p Dev: 0.309 r Dev: 0.757\n",
            "Epoch: 2500 Loss: 0.42536 Acc: 0.893 Acc Dev: 0.876 F1 Dev: 0.443 p Dev: 0.314 r Dev: 0.751\n",
            "Epoch: 3000 Loss: 0.42206 Acc: 0.896 Acc Dev: 0.879 F1 Dev: 0.442 p Dev: 0.316 r Dev: 0.734\n",
            "Epoch: 3500 Loss: 0.41778 Acc: 0.902 Acc Dev: 0.881 F1 Dev: 0.445 p Dev: 0.320 r Dev: 0.729\n",
            "Epoch: 4000 Loss: 0.41463 Acc: 0.907 Acc Dev: 0.883 F1 Dev: 0.450 p Dev: 0.326 r Dev: 0.729\n",
            "Epoch: 4500 Loss: 0.41024 Acc: 0.910 Acc Dev: 0.884 F1 Dev: 0.445 p Dev: 0.324 r Dev: 0.712\n",
            "Epoch: 5000 Loss: 0.40788 Acc: 0.910 Acc Dev: 0.886 F1 Dev: 0.445 p Dev: 0.326 r Dev: 0.701\n",
            "Epoch: 5500 Loss: 0.40396 Acc: 0.918 Acc Dev: 0.889 F1 Dev: 0.450 p Dev: 0.332 r Dev: 0.695\n",
            "Epoch: 6000 Loss: 0.40283 Acc: 0.918 Acc Dev: 0.889 F1 Dev: 0.444 p Dev: 0.331 r Dev: 0.678\n",
            "Epoch: 6500 Loss: 0.40054 Acc: 0.921 Acc Dev: 0.891 F1 Dev: 0.447 p Dev: 0.335 r Dev: 0.672\n",
            "Epoch: 7000 Loss: 0.39789 Acc: 0.925 Acc Dev: 0.893 F1 Dev: 0.452 p Dev: 0.340 r Dev: 0.672\n",
            "Epoch: 7500 Loss: 0.39596 Acc: 0.926 Acc Dev: 0.896 F1 Dev: 0.454 p Dev: 0.346 r Dev: 0.661\n",
            "Epoch: 8000 Loss: 0.39431 Acc: 0.930 Acc Dev: 0.896 F1 Dev: 0.449 p Dev: 0.344 r Dev: 0.644\n",
            "Epoch: 8500 Loss: 0.39194 Acc: 0.932 Acc Dev: 0.898 F1 Dev: 0.451 p Dev: 0.349 r Dev: 0.638\n",
            "Epoch: 9000 Loss: 0.39230 Acc: 0.933 Acc Dev: 0.901 F1 Dev: 0.458 p Dev: 0.358 r Dev: 0.638\n",
            "Epoch: 9500 Loss: 0.38881 Acc: 0.936 Acc Dev: 0.903 F1 Dev: 0.467 p Dev: 0.365 r Dev: 0.650\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGec8wtj7FcU",
        "colab_type": "code",
        "outputId": "e1fc745f-04c6-45a4-d7ac-8919c404effd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.sum(torch.max(outputs_dev, 1)[1].cpu().detach().numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRoJm6arNrvB",
        "colab_type": "code",
        "outputId": "b4be13a3-799d-46ca-a442-fef07eb7ba47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "# Create the network and get BCE loss\n",
        "net = Net()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#criterion = nn.CrossEntropyLoss(weight=[0.8, 0.2])\n",
        "# TODO: Make a SGD optimizer with lr=0.002 and momentum=0.99\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.05, momentum=0.99)\n",
        "# TODO: Move the net to the device\n",
        "net.to(device)\n",
        "\n",
        "# Let's do the same but with BATCHES\n",
        "x_train = x_train.to(device)\n",
        "y_train = y_train.to(device)\n",
        "x_test = x_test.to(device)\n",
        "y_test = y_test.to(device)\n",
        "x_zero = x_zero.to(device)\n",
        "x_one = x_one.to(device)\n",
        "net.train()\n",
        "losses = []\n",
        "accs = []\n",
        "accs_dev = []\n",
        "\n",
        "batch_size = 1000\n",
        "num_batches = int(np.ceil(len(x_train) / batch_size))\n",
        "for epoch in range(10000): \n",
        "  for i in range(num_batches):\n",
        "    start = i * batch_size\n",
        "    end = (i+1) * batch_size\n",
        "    \n",
        "    x_train_batch = torch.cat((x_one[torch.randperm(len(x_one))[0:500]], x_zero[torch.randperm(len(x_zero))[0:500]]), 0)\n",
        "    y_train_batch = torch.zeros_like(x_train_batch[:, 0], dtype=torch.long) \n",
        "    y_train_batch[0:500] = 1                  \n",
        "    y_train.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(x_train_batch)\n",
        "    loss = criterion(outputs, y_train_batch)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  if epoch % 500 == 0:\n",
        "      net.eval()\n",
        "      outputs = net(x_train)\n",
        "      acc = sklearn.metrics.accuracy_score([1 if x > 0.5 else 0 for x in torch.max(outputs, 1)[1].cpu().detach().numpy()], y_train.cpu().numpy())\n",
        "\n",
        "      outputs_dev = net(x_test)\n",
        "      f1_dev = f1_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      p_dev = precision_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      r_dev = recall_score(y_test.cpu().numpy(), torch.max(outputs_dev, 1)[1].cpu().detach().numpy())\n",
        "      \n",
        "      print(\"Epoch: {:4} Loss: {:.5f} Acc: {:.3f} Acc Dev: {:.3f} F1 Dev: {:.3f} p Dev: {:.3f} r Dev: {:.3f}\".format(epoch, loss.item(), acc, acc_dev, f1_dev, p_dev, r_dev))\n",
        "      net.train()\n",
        "print('Finished Training')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:    0 Loss: 0.69016 Acc: 0.924 Acc Dev: 0.934 F1 Dev: 0.011 p Dev: 1.000 r Dev: 0.006\n",
            "Epoch:  500 Loss: 0.38121 Acc: 0.929 Acc Dev: 0.934 F1 Dev: 0.644 p Dev: 0.494 r Dev: 0.927\n",
            "Epoch: 1000 Loss: 0.36795 Acc: 0.951 Acc Dev: 0.934 F1 Dev: 0.743 p Dev: 0.618 r Dev: 0.932\n",
            "Epoch: 1500 Loss: 0.37364 Acc: 0.964 Acc Dev: 0.934 F1 Dev: 0.793 p Dev: 0.690 r Dev: 0.932\n",
            "Epoch: 2000 Loss: 0.35429 Acc: 0.970 Acc Dev: 0.934 F1 Dev: 0.815 p Dev: 0.724 r Dev: 0.932\n",
            "Epoch: 2500 Loss: 0.35130 Acc: 0.975 Acc Dev: 0.934 F1 Dev: 0.838 p Dev: 0.760 r Dev: 0.932\n",
            "Epoch: 3000 Loss: 0.35295 Acc: 0.978 Acc Dev: 0.934 F1 Dev: 0.855 p Dev: 0.789 r Dev: 0.932\n",
            "Epoch: 3500 Loss: 0.35595 Acc: 0.980 Acc Dev: 0.934 F1 Dev: 0.862 p Dev: 0.801 r Dev: 0.932\n",
            "Epoch: 4000 Loss: 0.35849 Acc: 0.983 Acc Dev: 0.934 F1 Dev: 0.868 p Dev: 0.813 r Dev: 0.932\n",
            "Epoch: 4500 Loss: 0.34939 Acc: 0.984 Acc Dev: 0.934 F1 Dev: 0.885 p Dev: 0.842 r Dev: 0.932\n",
            "Epoch: 5000 Loss: 0.34665 Acc: 0.985 Acc Dev: 0.934 F1 Dev: 0.889 p Dev: 0.851 r Dev: 0.932\n",
            "Epoch: 5500 Loss: 0.35358 Acc: 0.986 Acc Dev: 0.934 F1 Dev: 0.902 p Dev: 0.873 r Dev: 0.932\n",
            "Epoch: 6000 Loss: 0.36020 Acc: 0.987 Acc Dev: 0.934 F1 Dev: 0.904 p Dev: 0.878 r Dev: 0.932\n",
            "Epoch: 6500 Loss: 0.34865 Acc: 0.987 Acc Dev: 0.934 F1 Dev: 0.904 p Dev: 0.878 r Dev: 0.932\n",
            "Epoch: 7000 Loss: 0.35080 Acc: 0.988 Acc Dev: 0.934 F1 Dev: 0.907 p Dev: 0.882 r Dev: 0.932\n",
            "Epoch: 7500 Loss: 0.35532 Acc: 0.988 Acc Dev: 0.934 F1 Dev: 0.912 p Dev: 0.892 r Dev: 0.932\n",
            "Epoch: 8000 Loss: 0.34637 Acc: 0.988 Acc Dev: 0.934 F1 Dev: 0.909 p Dev: 0.887 r Dev: 0.932\n",
            "Epoch: 8500 Loss: 0.34571 Acc: 0.989 Acc Dev: 0.934 F1 Dev: 0.909 p Dev: 0.887 r Dev: 0.932\n",
            "Epoch: 9000 Loss: 0.35219 Acc: 0.989 Acc Dev: 0.934 F1 Dev: 0.917 p Dev: 0.902 r Dev: 0.932\n",
            "Epoch: 9500 Loss: 0.34476 Acc: 0.989 Acc Dev: 0.934 F1 Dev: 0.919 p Dev: 0.907 r Dev: 0.932\n",
            "Finished Training\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}